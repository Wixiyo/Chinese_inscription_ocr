{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f52ae2d4-e181-4dbe-ae46-e2b9278355d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../')\n",
    "# 导入 opencv-python\n",
    "import cv2\n",
    "\n",
    "# 导入可视化工具包 matplotlib，并让绘制的图像嵌入在 notebook 中\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 定义可视化图像函数，输入图像路径，可视化图像\n",
    "def show_img_from_path(img_path):\n",
    "    \"\"\"opencv 读入图像，matplotlib 可视化格式为 RGB，因此需将 BGR 转 RGB，最后可视化出来\"\"\"\n",
    "    img = cv2.imread(img_path)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.show()\n",
    "\n",
    "# 定义可视化图像函数，输入图像 array，可视化图像\n",
    "def show_img_from_array(img):\n",
    "    \"\"\"输入 array，matplotlib 可视化格式为 RGB，因此需将 BGR 转 RGB，最后可视化出来\"\"\"\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.show()\n",
    "    \n",
    "show_img_from_path('../../data/imgs/u4e00_000048.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb394fea-8e37-49c4-b3f8-615ff02c87f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint_config = dict(interval=1)\n",
      "log_config = dict(interval=1000, hooks=[dict(type='TextLoggerHook')])\n",
      "dist_params = dict(backend='nccl')\n",
      "log_level = 'INFO'\n",
      "load_from = None\n",
      "resume_from = None\n",
      "workflow = [('train', 1)]\n",
      "opencv_num_threads = 0\n",
      "mp_start_method = 'fork'\n",
      "label_convertor = dict(\n",
      "    type='AttnConvertor', dict_type='DICT90', with_unknown=True)\n",
      "model = dict(\n",
      "    type='SARNet',\n",
      "    backbone=dict(type='ResNet31OCR'),\n",
      "    encoder=dict(\n",
      "        type='SAREncoder', enc_bi_rnn=False, enc_do_rnn=0.1, enc_gru=False),\n",
      "    decoder=dict(\n",
      "        type='ParallelSARDecoder',\n",
      "        enc_bi_rnn=False,\n",
      "        dec_bi_rnn=False,\n",
      "        dec_do_rnn=0,\n",
      "        dec_gru=False,\n",
      "        pred_dropout=0.1,\n",
      "        d_k=512,\n",
      "        pred_concat=True),\n",
      "    loss=dict(type='SARLoss'),\n",
      "    label_convertor=dict(\n",
      "        type='AttnConvertor', dict_type='DICT90', with_unknown=True),\n",
      "    max_seq_len=30)\n",
      "optimizer = dict(type='Adam', lr=0.000125)\n",
      "optimizer_config = dict(grad_clip=None)\n",
      "lr_config = dict(policy='step', step=[3, 4], warmup=None)\n",
      "total_epochs = 5\n",
      "img_norm_cfg = dict(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
      "train_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='ResizeOCR',\n",
      "        height=48,\n",
      "        min_width=48,\n",
      "        max_width=160,\n",
      "        keep_aspect_ratio=True,\n",
      "        width_downsample_ratio=0.25),\n",
      "    dict(type='ToTensorOCR'),\n",
      "    dict(type='NormalizeOCR', mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
      "    dict(\n",
      "        type='Collect',\n",
      "        keys=['img'],\n",
      "        meta_keys=[\n",
      "            'filename', 'ori_shape', 'resize_shape', 'text', 'valid_ratio'\n",
      "        ])\n",
      "]\n",
      "test_pipeline = [\n",
      "    dict(type='LoadImageFromFile'),\n",
      "    dict(\n",
      "        type='MultiRotateAugOCR',\n",
      "        rotate_degrees=[0, 90, 270],\n",
      "        transforms=[\n",
      "            dict(\n",
      "                type='ResizeOCR',\n",
      "                height=48,\n",
      "                min_width=48,\n",
      "                max_width=160,\n",
      "                keep_aspect_ratio=True,\n",
      "                width_downsample_ratio=0.25),\n",
      "            dict(type='ToTensorOCR'),\n",
      "            dict(\n",
      "                type='NormalizeOCR', mean=[0.5, 0.5, 0.5], std=[0.5, 0.5,\n",
      "                                                                0.5]),\n",
      "            dict(\n",
      "                type='Collect',\n",
      "                keys=['img'],\n",
      "                meta_keys=[\n",
      "                    'filename', 'ori_shape', 'resize_shape', 'valid_ratio',\n",
      "                    'img_norm_cfg', 'ori_filename', 'img_shape'\n",
      "                ])\n",
      "        ])\n",
      "]\n",
      "dataset_type = 'OCRDataset'\n",
      "root = '../../data'\n",
      "img_prefix = '../../data/imgs'\n",
      "train_anno_file1 = '../../data/train.txt'\n",
      "train1 = dict(\n",
      "    type='OCRDataset',\n",
      "    img_prefix='../../data/imgs',\n",
      "    ann_file='../../data/train.txt',\n",
      "    loader=dict(\n",
      "        type='HardDiskLoader',\n",
      "        repeat=100,\n",
      "        parser=dict(\n",
      "            type='LineStrParser',\n",
      "            keys=['filename', 'text'],\n",
      "            keys_idx=[0, 1],\n",
      "            separator=' ')),\n",
      "    pipeline=None,\n",
      "    test_mode=False)\n",
      "train_anno_file2 = '../../data/train.txt'\n",
      "train2 = dict(\n",
      "    type='OCRDataset',\n",
      "    img_prefix='../../data/imgs',\n",
      "    ann_file='../../data/train.txt',\n",
      "    loader=dict(\n",
      "        type='HardDiskLoader',\n",
      "        repeat=100,\n",
      "        parser=dict(\n",
      "            type='LineStrParser',\n",
      "            keys=['filename', 'text'],\n",
      "            keys_idx=[0, 1],\n",
      "            separator=' ')),\n",
      "    pipeline=None,\n",
      "    test_mode=False)\n",
      "test_anno_file1 = '../../data/test.txt'\n",
      "test = dict(\n",
      "    type='OCRDataset',\n",
      "    img_prefix='../../data/imgs',\n",
      "    ann_file='../../data/test.txt',\n",
      "    loader=dict(\n",
      "        type='HardDiskLoader',\n",
      "        repeat=1,\n",
      "        parser=dict(\n",
      "            type='LineStrParser',\n",
      "            keys=['filename', 'text'],\n",
      "            keys_idx=[0, 1],\n",
      "            separator=' ')),\n",
      "    pipeline=None,\n",
      "    test_mode=True)\n",
      "train_list = [\n",
      "    dict(\n",
      "        type='OCRDataset',\n",
      "        img_prefix='../../data/imgs',\n",
      "        ann_file='../../data/train.txt',\n",
      "        loader=dict(\n",
      "            type='HardDiskLoader',\n",
      "            repeat=100,\n",
      "            parser=dict(\n",
      "                type='LineStrParser',\n",
      "                keys=['filename', 'text'],\n",
      "                keys_idx=[0, 1],\n",
      "                separator=' ')),\n",
      "        pipeline=None,\n",
      "        test_mode=False),\n",
      "    dict(\n",
      "        type='OCRDataset',\n",
      "        img_prefix='../../data/imgs',\n",
      "        ann_file='../../data/train.txt',\n",
      "        loader=dict(\n",
      "            type='HardDiskLoader',\n",
      "            repeat=100,\n",
      "            parser=dict(\n",
      "                type='LineStrParser',\n",
      "                keys=['filename', 'text'],\n",
      "                keys_idx=[0, 1],\n",
      "                separator=' ')),\n",
      "        pipeline=None,\n",
      "        test_mode=False)\n",
      "]\n",
      "test_list = [\n",
      "    dict(\n",
      "        type='OCRDataset',\n",
      "        img_prefix='../../data/imgs',\n",
      "        ann_file='../../data/test.txt',\n",
      "        loader=dict(\n",
      "            type='HardDiskLoader',\n",
      "            repeat=1,\n",
      "            parser=dict(\n",
      "                type='LineStrParser',\n",
      "                keys=['filename', 'text'],\n",
      "                keys_idx=[0, 1],\n",
      "                separator=' ')),\n",
      "        pipeline=None,\n",
      "        test_mode=True)\n",
      "]\n",
      "data = dict(\n",
      "    workers_per_gpu=2,\n",
      "    samples_per_gpu=8,\n",
      "    train=dict(\n",
      "        type='UniformConcatDataset',\n",
      "        datasets=[\n",
      "            dict(\n",
      "                type='OCRDataset',\n",
      "                img_prefix='../../data/imgs',\n",
      "                ann_file='../../data/train.txt',\n",
      "                loader=dict(\n",
      "                    type='HardDiskLoader',\n",
      "                    repeat=100,\n",
      "                    parser=dict(\n",
      "                        type='LineStrParser',\n",
      "                        keys=['filename', 'text'],\n",
      "                        keys_idx=[0, 1],\n",
      "                        separator=' ')),\n",
      "                pipeline=None,\n",
      "                test_mode=False),\n",
      "            dict(\n",
      "                type='OCRDataset',\n",
      "                img_prefix='../../data/imgs',\n",
      "                ann_file='../../data/train.txt',\n",
      "                loader=dict(\n",
      "                    type='HardDiskLoader',\n",
      "                    repeat=100,\n",
      "                    parser=dict(\n",
      "                        type='LineStrParser',\n",
      "                        keys=['filename', 'text'],\n",
      "                        keys_idx=[0, 1],\n",
      "                        separator=' ')),\n",
      "                pipeline=None,\n",
      "                test_mode=False)\n",
      "        ],\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='ResizeOCR',\n",
      "                height=48,\n",
      "                min_width=48,\n",
      "                max_width=160,\n",
      "                keep_aspect_ratio=True,\n",
      "                width_downsample_ratio=0.25),\n",
      "            dict(type='ToTensorOCR'),\n",
      "            dict(\n",
      "                type='NormalizeOCR', mean=[0.5, 0.5, 0.5], std=[0.5, 0.5,\n",
      "                                                                0.5]),\n",
      "            dict(\n",
      "                type='Collect',\n",
      "                keys=['img'],\n",
      "                meta_keys=[\n",
      "                    'filename', 'ori_shape', 'resize_shape', 'text',\n",
      "                    'valid_ratio'\n",
      "                ])\n",
      "        ]),\n",
      "    val=dict(\n",
      "        type='UniformConcatDataset',\n",
      "        datasets=[\n",
      "            dict(\n",
      "                type='OCRDataset',\n",
      "                img_prefix='../../data/imgs',\n",
      "                ann_file='../../data/test.txt',\n",
      "                loader=dict(\n",
      "                    type='HardDiskLoader',\n",
      "                    repeat=1,\n",
      "                    parser=dict(\n",
      "                        type='LineStrParser',\n",
      "                        keys=['filename', 'text'],\n",
      "                        keys_idx=[0, 1],\n",
      "                        separator=' ')),\n",
      "                pipeline=None,\n",
      "                test_mode=True)\n",
      "        ],\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiRotateAugOCR',\n",
      "                rotate_degrees=[0, 90, 270],\n",
      "                transforms=[\n",
      "                    dict(\n",
      "                        type='ResizeOCR',\n",
      "                        height=48,\n",
      "                        min_width=48,\n",
      "                        max_width=160,\n",
      "                        keep_aspect_ratio=True,\n",
      "                        width_downsample_ratio=0.25),\n",
      "                    dict(type='ToTensorOCR'),\n",
      "                    dict(\n",
      "                        type='NormalizeOCR',\n",
      "                        mean=[0.5, 0.5, 0.5],\n",
      "                        std=[0.5, 0.5, 0.5]),\n",
      "                    dict(\n",
      "                        type='Collect',\n",
      "                        keys=['img'],\n",
      "                        meta_keys=[\n",
      "                            'filename', 'ori_shape', 'resize_shape',\n",
      "                            'valid_ratio', 'img_norm_cfg', 'ori_filename',\n",
      "                            'img_shape'\n",
      "                        ])\n",
      "                ])\n",
      "        ]),\n",
      "    test=dict(\n",
      "        type='UniformConcatDataset',\n",
      "        datasets=[\n",
      "            dict(\n",
      "                type='OCRDataset',\n",
      "                img_prefix='../../data/imgs',\n",
      "                ann_file='../../data/test.txt',\n",
      "                loader=dict(\n",
      "                    type='HardDiskLoader',\n",
      "                    repeat=1,\n",
      "                    parser=dict(\n",
      "                        type='LineStrParser',\n",
      "                        keys=['filename', 'text'],\n",
      "                        keys_idx=[0, 1],\n",
      "                        separator=' ')),\n",
      "                pipeline=None,\n",
      "                test_mode=True)\n",
      "        ],\n",
      "        pipeline=[\n",
      "            dict(type='LoadImageFromFile'),\n",
      "            dict(\n",
      "                type='MultiRotateAugOCR',\n",
      "                rotate_degrees=[0, 90, 270],\n",
      "                transforms=[\n",
      "                    dict(\n",
      "                        type='ResizeOCR',\n",
      "                        height=48,\n",
      "                        min_width=48,\n",
      "                        max_width=160,\n",
      "                        keep_aspect_ratio=True,\n",
      "                        width_downsample_ratio=0.25),\n",
      "                    dict(type='ToTensorOCR'),\n",
      "                    dict(\n",
      "                        type='NormalizeOCR',\n",
      "                        mean=[0.5, 0.5, 0.5],\n",
      "                        std=[0.5, 0.5, 0.5]),\n",
      "                    dict(\n",
      "                        type='Collect',\n",
      "                        keys=['img'],\n",
      "                        meta_keys=[\n",
      "                            'filename', 'ori_shape', 'resize_shape',\n",
      "                            'valid_ratio', 'img_norm_cfg', 'ori_filename',\n",
      "                            'img_shape'\n",
      "                        ])\n",
      "                ])\n",
      "        ]))\n",
      "evaluation = dict(interval=1, metric='acc')\n",
      "work_dir = './demo/recog'\n",
      "seed = 0\n",
      "gpu_ids = range(0, 1)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from mmcv import Config\n",
    "from mmdet.apis import set_random_seed\n",
    "\n",
    "cfg = Config.fromfile('./configs/textrecog/sar/sar_r31_parallel_decoder_stone_dataset.py')\n",
    "\n",
    "# 存放输出结果和日志的目录\n",
    "cfg.work_dir = './demo/recog'\n",
    "\n",
    "# 初始学习率 0.001 是针对 8 个 GPU 训练的\n",
    "# 如果只有一个 GPU，则除以8\n",
    "cfg.optimizer.lr = 0.001 / 8\n",
    "cfg.lr_config.warmup = None\n",
    "\n",
    "# 每训练40张图像，记录一次日志\n",
    "cfg.log_config.interval = 1000\n",
    "\n",
    "# 设置随机数种子\n",
    "cfg.seed = 0\n",
    "set_random_seed(0, deterministic=False)\n",
    "cfg.gpu_ids = range(1)\n",
    "\n",
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a990ced-9627-4122-982c-fab0fc613625",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/featurize/work/mmocr/mmocr/datasets/utils/loader.py:86: UserWarning: HardDiskLoader is deprecated, please use AnnFileLoader instead.\n",
      "  'AnnFileLoader instead.', UserWarning)\n",
      "/home/featurize/work/mmocr/mmocr/apis/train.py:86: UserWarning: config is now expected to have a `runner` section, please set `runner` in your config.\n",
      "  'please set `runner` in your config.', UserWarning)\n",
      "/home/featurize/work/mmocr/mmocr/apis/utils.py:53: UserWarning: Remove \"MultiRotateAugOCR\" to support batch inference since samples_per_gpu > 1.\n",
      "  warnings.warn(warning_msg)\n",
      "/home/featurize/work/mmocr/mmocr/datasets/utils/loader.py:86: UserWarning: HardDiskLoader is deprecated, please use AnnFileLoader instead.\n",
      "  'AnnFileLoader instead.', UserWarning)\n",
      "2022-05-04 00:40:35,613 - mmocr - INFO - Start running, host: featurize@featurize, work_dir: /home/featurize/work/mmocr/demo/recog\n",
      "2022-05-04 00:40:35,614 - mmocr - INFO - Hooks will be executed in the following order:\n",
      "before_run:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_epoch:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_train_iter:\n",
      "(VERY_HIGH   ) StepLrUpdaterHook                  \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_train_iter:\n",
      "(ABOVE_NORMAL) OptimizerHook                      \n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_train_epoch:\n",
      "(NORMAL      ) CheckpointHook                     \n",
      "(NORMAL      ) EvalHook                           \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_epoch:\n",
      "(LOW         ) IterTimerHook                      \n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "before_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_iter:\n",
      "(LOW         ) IterTimerHook                      \n",
      " -------------------- \n",
      "after_val_epoch:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "after_run:\n",
      "(VERY_LOW    ) TextLoggerHook                     \n",
      " -------------------- \n",
      "2022-05-04 00:40:35,615 - mmocr - INFO - workflow: [('train', 1)], max: 5 epochs\n",
      "2022-05-04 00:40:35,616 - mmocr - INFO - Checkpoints will be saved to /home/featurize/work/mmocr/demo/recog by HardDiskBackend.\n",
      "2022-05-04 00:40:45,893 - mmocr - INFO - Epoch [1][40/1007275]\tlr: 1.250e-04, eta: 14 days, 22:38:34, time: 0.256, data_time: 0.112, memory: 1781, loss_ce: 2.9914, loss: 2.9914\n",
      "2022-05-04 00:40:51,337 - mmocr - INFO - Epoch [1][80/1007275]\tlr: 1.250e-04, eta: 11 days, 10:31:28, time: 0.136, data_time: 0.006, memory: 1781, loss_ce: 2.4570, loss: 2.4570\n",
      "2022-05-04 00:40:56,622 - mmocr - INFO - Epoch [1][120/1007275]\tlr: 1.250e-04, eta: 10 days, 4:37:32, time: 0.132, data_time: 0.002, memory: 1781, loss_ce: 2.2966, loss: 2.2966\n",
      "2022-05-04 00:41:01,926 - mmocr - INFO - Epoch [1][160/1007275]\tlr: 1.250e-04, eta: 9 days, 13:49:57, time: 0.133, data_time: 0.002, memory: 1781, loss_ce: 2.2885, loss: 2.2885\n",
      "2022-05-04 00:41:07,223 - mmocr - INFO - Epoch [1][200/1007275]\tlr: 1.250e-04, eta: 9 days, 4:55:07, time: 0.132, data_time: 0.002, memory: 1781, loss_ce: 2.2432, loss: 2.2432\n",
      "2022-05-04 00:41:12,533 - mmocr - INFO - Epoch [1][240/1007275]\tlr: 1.250e-04, eta: 8 days, 23:02:38, time: 0.133, data_time: 0.002, memory: 1781, loss_ce: 2.2401, loss: 2.2401\n",
      "2022-05-04 00:41:17,850 - mmocr - INFO - Epoch [1][280/1007275]\tlr: 1.250e-04, eta: 8 days, 18:53:04, time: 0.133, data_time: 0.002, memory: 1781, loss_ce: 2.2191, loss: 2.2191\n",
      "2022-05-04 00:41:23,169 - mmocr - INFO - Epoch [1][320/1007275]\tlr: 1.250e-04, eta: 8 days, 15:46:15, time: 0.133, data_time: 0.002, memory: 1781, loss_ce: 2.2426, loss: 2.2426\n",
      "2022-05-04 00:41:28,499 - mmocr - INFO - Epoch [1][360/1007275]\tlr: 1.250e-04, eta: 8 days, 13:23:39, time: 0.133, data_time: 0.002, memory: 1781, loss_ce: 2.2448, loss: 2.2448\n",
      "2022-05-04 00:41:33,822 - mmocr - INFO - Epoch [1][400/1007275]\tlr: 1.250e-04, eta: 8 days, 11:28:09, time: 0.133, data_time: 0.002, memory: 1781, loss_ce: 2.1991, loss: 2.1991\n",
      "2022-05-04 00:41:39,137 - mmocr - INFO - Epoch [1][440/1007275]\tlr: 1.250e-04, eta: 8 days, 9:52:01, time: 0.133, data_time: 0.002, memory: 1781, loss_ce: 2.2279, loss: 2.2279\n",
      "2022-05-04 00:41:44,441 - mmocr - INFO - Epoch [1][480/1007275]\tlr: 1.250e-04, eta: 8 days, 8:29:50, time: 0.133, data_time: 0.002, memory: 1781, loss_ce: 2.2379, loss: 2.2379\n",
      "2022-05-04 00:41:49,731 - mmocr - INFO - Epoch [1][520/1007275]\tlr: 1.250e-04, eta: 8 days, 7:18:12, time: 0.132, data_time: 0.002, memory: 1781, loss_ce: 2.2069, loss: 2.2069\n",
      "2022-05-04 00:41:55,019 - mmocr - INFO - Epoch [1][560/1007275]\tlr: 1.250e-04, eta: 8 days, 6:16:30, time: 0.132, data_time: 0.002, memory: 1781, loss_ce: 2.1847, loss: 2.1847\n",
      "2022-05-04 00:42:00,333 - mmocr - INFO - Epoch [1][600/1007275]\tlr: 1.250e-04, eta: 8 days, 5:26:29, time: 0.133, data_time: 0.003, memory: 1781, loss_ce: 2.1811, loss: 2.1811\n",
      "2022-05-04 00:42:05,741 - mmocr - INFO - Epoch [1][640/1007275]\tlr: 1.250e-04, eta: 8 days, 4:55:11, time: 0.135, data_time: 0.005, memory: 1781, loss_ce: 2.1914, loss: 2.1914\n",
      "2022-05-04 00:42:11,194 - mmocr - INFO - Epoch [1][680/1007275]\tlr: 1.250e-04, eta: 8 days, 4:33:07, time: 0.136, data_time: 0.006, memory: 1781, loss_ce: 2.2160, loss: 2.2160\n",
      "2022-05-04 00:42:17,458 - mmocr - INFO - Epoch [1][720/1007275]\tlr: 1.250e-04, eta: 8 days, 5:47:55, time: 0.157, data_time: 0.026, memory: 1781, loss_ce: 2.1952, loss: 2.1952\n",
      "2022-05-04 00:42:22,875 - mmocr - INFO - Epoch [1][760/1007275]\tlr: 1.250e-04, eta: 8 days, 5:21:24, time: 0.135, data_time: 0.005, memory: 1781, loss_ce: 2.1488, loss: 2.1488\n",
      "2022-05-04 00:42:28,374 - mmocr - INFO - Epoch [1][800/1007275]\tlr: 1.250e-04, eta: 8 days, 5:06:01, time: 0.137, data_time: 0.007, memory: 1781, loss_ce: 2.1674, loss: 2.1674\n",
      "2022-05-04 00:42:33,887 - mmocr - INFO - Epoch [1][840/1007275]\tlr: 1.250e-04, eta: 8 days, 4:53:33, time: 0.138, data_time: 0.008, memory: 1781, loss_ce: 2.1873, loss: 2.1873\n",
      "2022-05-04 00:42:39,602 - mmocr - INFO - Epoch [1][880/1007275]\tlr: 1.250e-04, eta: 8 days, 5:01:33, time: 0.143, data_time: 0.013, memory: 1781, loss_ce: 2.1691, loss: 2.1691\n",
      "2022-05-04 00:42:44,922 - mmocr - INFO - Epoch [1][920/1007275]\tlr: 1.250e-04, eta: 8 days, 4:32:41, time: 0.133, data_time: 0.003, memory: 1781, loss_ce: 2.1545, loss: 2.1545\n",
      "2022-05-04 00:42:50,311 - mmocr - INFO - Epoch [1][960/1007275]\tlr: 1.250e-04, eta: 8 days, 4:12:22, time: 0.135, data_time: 0.004, memory: 1781, loss_ce: 2.1285, loss: 2.1285\n",
      "2022-05-04 00:42:55,725 - mmocr - INFO - Epoch [1][1000/1007275]\tlr: 1.250e-04, eta: 8 days, 3:55:38, time: 0.135, data_time: 0.005, memory: 1781, loss_ce: 2.1628, loss: 2.1628\n",
      "2022-05-04 00:43:01,000 - mmocr - INFO - Epoch [1][1040/1007275]\tlr: 1.250e-04, eta: 8 days, 3:29:05, time: 0.132, data_time: 0.002, memory: 1781, loss_ce: 2.1311, loss: 2.1311\n",
      "2022-05-04 00:43:06,277 - mmocr - INFO - Epoch [1][1080/1007275]\tlr: 1.250e-04, eta: 8 days, 3:04:35, time: 0.132, data_time: 0.002, memory: 1781, loss_ce: 2.1428, loss: 2.1428\n",
      "2022-05-04 00:43:11,556 - mmocr - INFO - Epoch [1][1120/1007275]\tlr: 1.250e-04, eta: 8 days, 2:42:01, time: 0.132, data_time: 0.002, memory: 1781, loss_ce: 2.1797, loss: 2.1797\n",
      "2022-05-04 00:43:16,846 - mmocr - INFO - Epoch [1][1160/1007275]\tlr: 1.250e-04, eta: 8 days, 2:21:43, time: 0.132, data_time: 0.002, memory: 1781, loss_ce: 2.1925, loss: 2.1925\n"
     ]
    }
   ],
   "source": [
    "import mmcv\n",
    "from mmocr.datasets import build_dataset\n",
    "from mmocr.models import build_detector\n",
    "from mmocr.apis import train_detector\n",
    "import os.path as osp\n",
    "\n",
    "# 建立数据集\n",
    "datasets = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# 建立模型\n",
    "model = build_detector(cfg.model, train_cfg=cfg.get('train_cfg'), test_cfg=cfg.get('test_cfg'))\n",
    "\n",
    "# 创建新目录，保存训练结果\n",
    "mmcv.mkdir_or_exist(osp.abspath(cfg.work_dir))\n",
    "\n",
    "train_detector(model, datasets, cfg, distributed=False, validate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69bb060-ce50-497b-b879-6abaced78ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmocr.apis import init_detector, model_inference\n",
    "# 指定模型 checkpoint 权重文件\n",
    "checkpoint = \"./demo/tutorial_exps/epoch_5.pth\"\n",
    "\n",
    "\n",
    "model = init_detector(cfg, checkpoint, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e11a532-5b4c-4b0a-8df9-fc5d11eec6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定待测图像路径\n",
    "# input_path = '../../data/imgs/u4e00_000058.jpg'\n",
    "input_path = './demo/input/1 (440).jpg'\n",
    "result = model_inference(model, input_path)\n",
    "bs = '\\\\'+result['text']\n",
    "print(bs.encode('utf-8').decode('unicode_escape'))\n",
    "out_img = model.show_result(input_path, result, out_file='outputs/demo-out.jpg', show=False)\n",
    "show_img_from_array(out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95df3fb5-6bec-4af3-bebe-6e2cb732d8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
